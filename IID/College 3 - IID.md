# College 3 - IID

## Recapitulation

What is really different? Changes in the method, often very large-scale methods that are open shared and distributed. Also there are differences in objects and presentations.

**METHOD, OBJECT, PRESENTATION**

## Methodology

Why a method? We use methods because ... well. We chose the method that is the most apropriate (?) (ohw). We pick the one that yields the most interesting interstings things from intereseting points of view. 

What are the differences between Sciences and the Humanities. Science claim a truth-finding paradigm. Humanties claim a interpretative paradigm. Its a view from german philosophers. Its not this black and white. There seems to be a fundamental difference. DH sometime claims a combination of the two. 

Empirical research has a clear method. scientific method. 
1. Hypotheses
2. Method
3. Results
4. Conclusion

This is again a very old way of working. PHD-comics. There isn't a perfect scientific method. 

Competing Paradigms. There is often a clash. Are they mutally exclusive? Can interpretations be also be viewed as patterns? Can there be a more empirical apprach to interpretation? Can interpretation be collaborative?

Digital Interpretation? Can digital methods transform/affect interpretation? Can digital methods do interpretation for you? (loaded question) Soms methods accually try. To which paradigm is you way of using DH?

Distinguishing beween qualitative and quantative methods. To which is your method closer? Does DH offer new methods for either of these? Often quantative, qualatative is more questionable.

## Distant reading and interpretation

Close reading is analysing every relation within in the text. It  takes alot of time! Scruitinising every sentence and word. You can do different kinds of reading. Skimming, or triage, is also a technique. Immersion is also a way, getting lost in the text, losing abstract reflection. 

The very notion of close reading has been recently been opposed to distant reading. The total number of book every published gets close to 129 million, alot are digitised. Franko Moretti coined the term.

The very idea behind it is information overload. There was more than what people could read in the Library of Alexandria already. 700.000 manuscripts. (or 400.000) The invention of the printingpress made it worse and worse. Techniques like triage were invented. 

If we look at certain parts of literature, like fiction, you could still read everything arround 1770. Yet after that there is an explosion. You can't keep up. What people usually do is read a selection. That selection is claimed to be the 'canon' of books. But who decides it? At the time there were few alternatives to this. 

Moretti tried to consider other methods. Abstracts representations of texts should be read, not the text themselves. He analysed a play of Shakespeare by only reading parts. You look at the more general patterns in books, over larger ammounts of books. Literary scholars don't like this. You find analogies in other fields. 

Larger patterns -> conjectures (in between) -> and single events. 

Every novel is individual, but more books might contain a pattern. This is where digital humanities might be very usefull.  What is de affordance of quantative methods. So you try to look at the data without interpretation (although that is impossible). Pablo Picasso once stated that computers are worthless because they only give anwsers, not questions. Patterns and interpretations are never seperated, only by looking for it you're already interpreting. But the idea is to stay as impartial as ever possible. 

## Culturomics

Culturomics is a field in which quantative analysis of culture using millions of digitised books. Googles NGRAM viewer is a interesting example. It counts sequences of words. The idea of culturomics is to widely analyse human culture by looking into books. Close to the idea to distant reading. Counting instead of reading. The first thing they want to do in the TED movie is interpret once they've found a pattern. 

TED: author -> book -> library -> data (with metadata).
There could be many more critisisms. 

## Digital identification of patterns

Understanding the Ngram-viewer:
We can look at patterns. But can we thrust the diagram? They have only gotten the books from libraries who wanted to cooperate. Less of an issue though as more books are scanned. There are errors in the OCR too. But there are interesting examples. Google is commericial, and doesn't make their data open. They add millions of books too. Research should be replicatable, which it isn't with the Ngram vieuwer. Things change. 

Counting versus interpretation? 
Meaning and frequency. The disappearance of the the word God from society you could draw conclusions from it. But only as basis for furhter research.

Raymond Williams has worked alot with discovering patterns. For instance the patterns of advertising in television. He found that its though to turn those patterns into meaning. Could reading techniques create a shift in meaning?

Stanly Fish has claimed that the humanities are useless. But he also said, "Don't we have to read the books before saying what the patterns really mean?" 

Stephen Ramsay is about combining computation text analysis and literary critisism at the same time? Is it meaningfull? He makes a interesing point. Patterns urge interpretations that bring forth patterns. 

Both are possible. 

But what are patterns? before we go further with interpretations? 

## Break

## Guest lecture -> Piek Vossen

Annotation Man versus Machine. Another module that is in the minor. Are there differences? Its not only practical things, also fundamental.

Annotation = assigning meaning to signals, turning signals into data. There are alot of questions in the start. Why do we care? There are a huge ammount of pratical problems related to this too. We select already: Observe -> Signals -> Data.

New reader. A project. There are millions of articles produced every day, that try to inform you about what's going on. There are 4 newsfragments in de slide. They show the same subject. You cannot interpret this without alot of background knowledge. 

Computers try to read this text. The computer should distill simple information. What, whom and when. ITs not so easy to find out that they are talking about the same thing in each message. What do the media tell us about this process. Its difficult for computers Ã¡nd humans. There is alot of opinion hidden in there aswell. How do you decide what the opinion is? People will differ alot in the interpretation. 

"Belubberen" we cannot annotate it, we cannot interpret it. It is a dutch word though. There is a theory that the context determines the meaning. This is an extreme case where that does not easily apply. It's not easy to figure out. Lubbers was applying a nononsense policy, in a crisis. Pepole did'nt like it. 

## Hermeneutic Circle

The only way to get the meaning of a word truly you need to examine it in a rich context, not just the neigbouring words. Its subjective by definition. Assigning meaning to sources, objects, symbols, data in a context. You need a socio-cultural and cognitive context.

There is something changing though. Alot of people believe that. Alot of the events that are happening are being registered. Everything is recorded (although in a flawd and biased way). Signals are complicated, can refer to multiple things. We make explicit what a term refer too. Things like wikipedia become an anchorpoint in meaning. A tool to which we structure the world. 

DBpedia is like an api for wikipedia.

Were using a reference. There is ambiguity in terms and synonymy. There is no proof. Why do we think in certain terms? because someone came up with it. We converge more and more to a representation to world were mapping too.

(what is language? -> tool for structuring)

## DH

There are too many books, news, sources. Who can check all this?  Opinions are being pushed arround the world and we cannot control it. Its unavoidable. It also applies historical data and archives. What can you observe? A small fragment, but thats not representative. On what selection do we base our analysis? You cannot read everything. You cannot check all the references?  Its madness. 

A researcher puts it in a monograph that might evolve to a theory. That is the way the humanties have been working a long time. Its good and well, but DH can add to this. Codebook, model tagset, annotation module. 

There is a newscompany that already opens issues before issues arise. When politicians ask, they can provide. They produce a document on which the parliament takes a decision. Nobody can trace wheter they are correct. 

You can do whatever you want, but make it explicit where you got the data from. Everybody has to be able to trace data back. You have a theory with notions. Notion: "if this, then that applies". These you put in a codebook. You tag it. You make it transparent, not objective.

Manual annotation can be a trainingmodel for automatic analysis. As a researcher you run into problems that your data is noisy. You will never find everything. Computers will build upon it and layer new extra information on top of it. What will it mean that things are missing, you need to know a bit about that. 

Computers can create association between low leven contextual features and assigned meta data. Can a computer annotate large volumes of data correctly? Can a computer model the human way of assigning meaning? They cannot understand identity. 

(how do we do that? Our understanding is shaped by our humanity. We're a simple computer too in the end.)

The point is that metadata is text and labels (text) linked to sources. We want to formalise meta-data somehow. In the symanantic webcommunity its a big thing. **Linked data** LOD cloud. You see a rond circle, and that's a database. You can consider this web as the internet. 31 billion fact. No every database knows everything, but linked ones know alot more. RDF is the format used. All of this needs to be interpreted. We have to annotate it with metadata, but that has to be done in a formalised structure. Combining all knowledge in a smart way. You can make alot of connections. 

## Rise of the digital data

Data used to be distributed in some proprietary format. only internally accesable. Non standard. This has to be linked accesable to everyone, everywhere. 

## Dimensions of big data

There is alot of data, but the data itself is very limited. People a limited in their capacity to feed a database with data.  We want to go from nothing from nobody to everything from everybody... (?) 

## Machines assigning meaning

Newsreader project assigns some meaning to news streams. It doesn't detect alot, but it does detect when, where and whom.  Google generally doesn't interpret, but only counts volumes. Computers can do this pretty well. 

They can detect the sentiment however.(?)

BiographyNet is also an initiative. Its about developing new tools and technology to do historical research. Nobody knows what the tools are. You have to be very carefull. Historical and computational thinking. It's another way of doing research and analysis. They took the biography portal of the Netherlands (there are 110.000 biographys) Its a fusion of a lot dictionairies. Some written ages ago, some recent. The concept of a biography changed over time. Biographynet tried to assign metadata to it. You can do some analysis on it now. You can view in the enrichment example how they did it.  They use natural language processing to detemine the metadata. For this they state the program used, version and creator. It makes explicit where the data comes from. 


## Conclusion 

Too much data for humans to process. Open linked humanites data is important to standardise it. It opens up new ways of combining and analysing data. This also requires a new type of researcher, and this kind is more employable.

Were making the subjectivity analysable. Were not disqualifing old research. 

You can measure subjectivity in consitance. Often people can only agree on high level, details are aways tricky. 

Pattern -> annotations -> interpretation

